{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "This notebook demonstrates the workflow for building and training a Variational Autoencoder (VAE) using PyTorch with the MNIST dataset. This notebook will walk through the key steps, including data preparation, model definition, training, and evaluation, providing a comprehensive guide to implementing VAEs for unsupervised learning tasks on handwritten digit images.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries\n",
    "\n",
    "Library | Version | Channel\n",
    "--- | --- | ---\n",
    "NumPy | 1.26.4 | default\n",
    "PyTorch | 2.2.2 | pytorch\n",
    "Torchvision | 0.17.2 | pytorch\n",
    "Tensorboard | / | conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The [MNIST](http://yann.lecun.com/exdb/mnist/) dataset is a widely-used benchmark in machine learning, consisting of 70,000 images of handwritten digits from 0 to 9. Each image is a 28x28 grayscale pixel grid. Due to its simplicity and well-structured format, MNIST serves as an excellent starting point for developing and testing machine learning models, particularly in the field of image recognition and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size\n",
    "input_layer = 784\n",
    "layer_one = 784 // 2\n",
    "layer_two = 784 // 4\n",
    "layer_three = 784 // 8\n",
    "latent_space = 4\n",
    "# Dataloaders\n",
    "batch_size = 128\n",
    "# Optimizer\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-2\n",
    "# Training\n",
    "folds = 5\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "The **Variational Autoencoder (VAE)** was firstly introduced by Kingma and Welling in 2013 [[1]](https://arxiv.org/abs/1312.6114). A VAE is a generative model comprising an **encoder**, which maps the input data to a latent space. This component is also referred to as a recognition model, as it is responsible for recognising important patterns within the data. The other component of the model is a **decoder**, which generates a reconstructed representation of the input data. This is why the decoder is also referred to as the generative model [[2]](https://hunterheidenreich.com/posts/modern-variational-autoencoder-in-pytorch/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Lambda(lambda x: x.view(-1) - 0.5),\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "train_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=True, \n",
    "    transform=transform,\n",
    ")\n",
    "# Download and load the test data\n",
    "test_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VariationalAutoencoder(input_layer, layer_one, layer_two, layer_three, latent_space).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "writer = SummaryWriter(f'runs/mnist/vae_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, prev_updates, writer=None):\n",
    "    \"\"\"\n",
    "    Trains the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        loss_fn: The loss function.\n",
    "        optimizer: The optimizer.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        output = model(data)  # Forward pass\n",
    "        loss = output.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if n_upd % 100 == 0:\n",
    "            # Calculate and log gradient norms\n",
    "            total_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "            print(f'Step {n_upd:,} (N samples: {n_upd*batch_size:,}), Loss: {loss.item():.4f} (Recon: {output.loss_recon.item():.4f}, KL: {output.loss_kl.item():.4f}) Grad: {total_norm:.4f}')\n",
    "\n",
    "            if writer is not None:\n",
    "                global_step = n_upd\n",
    "                writer.add_scalar('Loss/Train', loss.item(), global_step)\n",
    "                writer.add_scalar('Loss/Train/BCE', output.loss_recon.item(), global_step)\n",
    "                writer.add_scalar('Loss/Train/KLD', output.loss_kl.item(), global_step)\n",
    "                writer.add_scalar('GradNorm/Train', total_norm, global_step)\n",
    "            \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        \n",
    "        optimizer.step()  # Update the model parameters\n",
    "        \n",
    "    return prev_updates + len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, cur_step, writer=None):\n",
    "    \"\"\"\n",
    "    Tests the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to test.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        cur_step (int): The current step.\n",
    "        writer: The TensorBoard writer.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    test_recon_loss = 0\n",
    "    test_kl_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data = data.to(device)\n",
    "            data = data.view(data.size(0), -1)  # Flatten the data\n",
    "            \n",
    "            output = model(data, compute_loss=True)  # Forward pass\n",
    "            \n",
    "            test_loss += output.loss.item()\n",
    "            test_recon_loss += output.loss_recon.item()\n",
    "            test_kl_loss += output.loss_kl.item()\n",
    "            \n",
    "    test_loss /= len(dataloader)\n",
    "    test_recon_loss /= len(dataloader)\n",
    "    test_kl_loss /= len(dataloader)\n",
    "    print(f'====> Test set loss: {test_loss:.4f} (BCE: {test_recon_loss:.4f}, KLD: {test_kl_loss:.4f})')\n",
    "    \n",
    "    if writer is not None:\n",
    "        writer.add_scalar('Loss/Test', test_loss, global_step=cur_step)\n",
    "        writer.add_scalar('Loss/Test/BCE', output.loss_recon.item(), global_step=cur_step)\n",
    "        writer.add_scalar('Loss/Test/KLD', output.loss_kl.item(), global_step=cur_step)\n",
    "        \n",
    "        # Log reconstructions\n",
    "        writer.add_images('Test/Reconstructions', output.x_recon.view(-1, 1, 28, 28), global_step=cur_step)\n",
    "        writer.add_images('Test/Originals', data.view(-1, 1, 28, 28), global_step=cur_step)\n",
    "        \n",
    "        # Log random samples from the latent space\n",
    "        z = torch.randn(16, latent_space).to(device)\n",
    "        samples = model.decode(z)\n",
    "        writer.add_images('Test/Samples', samples.view(-1, 1, 28, 28), global_step=cur_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Step 0 (N samples: 0), Loss: 164.3871 (Recon: 159.9528, KL: 4.4342) Grad: 29.7743\n",
      "Step 100 (N samples: 12,800), Loss: 156.7009 (Recon: 150.5463, KL: 6.1546) Grad: 23.8390\n",
      "Step 200 (N samples: 25,600), Loss: 166.9792 (Recon: 160.3606, KL: 6.6186) Grad: 64.0568\n",
      "Step 300 (N samples: 38,400), Loss: 152.1218 (Recon: 145.3236, KL: 6.7981) Grad: 39.0422\n",
      "Step 400 (N samples: 51,200), Loss: 145.5018 (Recon: 138.7176, KL: 6.7842) Grad: 30.9859\n",
      "====> Test set loss: 146.6436 (BCE: 139.5255, KLD: 7.1181)\n",
      "Epoch 2/15\n",
      "Step 500 (N samples: 64,000), Loss: 147.8998 (Recon: 140.7776, KL: 7.1221) Grad: 38.2662\n",
      "Step 600 (N samples: 76,800), Loss: 145.5654 (Recon: 137.3705, KL: 8.1949) Grad: 45.3294\n",
      "Step 700 (N samples: 89,600), Loss: 140.7345 (Recon: 132.3809, KL: 8.3535) Grad: 31.1977\n",
      "Step 800 (N samples: 102,400), Loss: 131.4105 (Recon: 122.7430, KL: 8.6675) Grad: 34.2402\n",
      "Step 900 (N samples: 115,200), Loss: 136.9081 (Recon: 127.9592, KL: 8.9490) Grad: 44.5080\n",
      "====> Test set loss: 134.6257 (BCE: 125.4683, KLD: 9.1574)\n",
      "Epoch 3/15\n",
      "Step 1,000 (N samples: 128,000), Loss: 140.4751 (Recon: 131.4272, KL: 9.0480) Grad: 41.6917\n",
      "Step 1,100 (N samples: 140,800), Loss: 136.2196 (Recon: 126.6835, KL: 9.5360) Grad: 45.4555\n",
      "Step 1,200 (N samples: 153,600), Loss: 130.5345 (Recon: 121.2489, KL: 9.2856) Grad: 46.3298\n",
      "Step 1,300 (N samples: 166,400), Loss: 128.6715 (Recon: 119.3613, KL: 9.3101) Grad: 48.3148\n",
      "Step 1,400 (N samples: 179,200), Loss: 130.9395 (Recon: 121.5273, KL: 9.4122) Grad: 58.2297\n",
      "====> Test set loss: 130.3347 (BCE: 121.0588, KLD: 9.2759)\n",
      "Epoch 4/15\n",
      "Step 1,500 (N samples: 192,000), Loss: 126.0806 (Recon: 116.5787, KL: 9.5019) Grad: 40.9763\n",
      "Step 1,600 (N samples: 204,800), Loss: 127.4825 (Recon: 117.7293, KL: 9.7531) Grad: 34.9549\n",
      "Step 1,700 (N samples: 217,600), Loss: 129.7110 (Recon: 120.2317, KL: 9.4792) Grad: 40.1730\n",
      "Step 1,800 (N samples: 230,400), Loss: 131.7435 (Recon: 122.2410, KL: 9.5026) Grad: 38.8048\n",
      "====> Test set loss: 127.8682 (BCE: 118.0912, KLD: 9.7770)\n",
      "Epoch 5/15\n",
      "Step 1,900 (N samples: 243,200), Loss: 126.5771 (Recon: 116.8941, KL: 9.6830) Grad: 49.1284\n",
      "Step 2,000 (N samples: 256,000), Loss: 127.8142 (Recon: 118.1191, KL: 9.6951) Grad: 50.2405\n",
      "Step 2,100 (N samples: 268,800), Loss: 136.9212 (Recon: 127.1249, KL: 9.7963) Grad: 43.9752\n",
      "Step 2,200 (N samples: 281,600), Loss: 122.3089 (Recon: 112.7036, KL: 9.6053) Grad: 32.8424\n",
      "Step 2,300 (N samples: 294,400), Loss: 125.6290 (Recon: 115.9085, KL: 9.7205) Grad: 49.5358\n",
      "====> Test set loss: 126.4563 (BCE: 116.6581, KLD: 9.7982)\n",
      "Epoch 6/15\n",
      "Step 2,400 (N samples: 307,200), Loss: 123.3156 (Recon: 113.5888, KL: 9.7268) Grad: 47.2894\n",
      "Step 2,500 (N samples: 320,000), Loss: 127.2767 (Recon: 117.3170, KL: 9.9598) Grad: 39.4564\n",
      "Step 2,600 (N samples: 332,800), Loss: 121.9285 (Recon: 111.9779, KL: 9.9507) Grad: 37.8012\n",
      "Step 2,700 (N samples: 345,600), Loss: 124.8410 (Recon: 114.7276, KL: 10.1134) Grad: 39.0158\n",
      "Step 2,800 (N samples: 358,400), Loss: 127.5793 (Recon: 117.6511, KL: 9.9282) Grad: 53.7349\n",
      "====> Test set loss: 124.9040 (BCE: 114.8560, KLD: 10.0480)\n",
      "Epoch 7/15\n",
      "Step 2,900 (N samples: 371,200), Loss: 127.4887 (Recon: 117.6493, KL: 9.8394) Grad: 40.4820\n",
      "Step 3,000 (N samples: 384,000), Loss: 124.1116 (Recon: 114.0786, KL: 10.0330) Grad: 36.8169\n",
      "Step 3,100 (N samples: 396,800), Loss: 117.6329 (Recon: 107.4216, KL: 10.2113) Grad: 49.7579\n",
      "Step 3,200 (N samples: 409,600), Loss: 123.6944 (Recon: 113.4711, KL: 10.2233) Grad: 43.0015\n",
      "====> Test set loss: 123.7821 (BCE: 113.6513, KLD: 10.1308)\n",
      "Epoch 8/15\n",
      "Step 3,300 (N samples: 422,400), Loss: 122.8882 (Recon: 112.8513, KL: 10.0369) Grad: 44.5767\n",
      "Step 3,400 (N samples: 435,200), Loss: 123.2439 (Recon: 113.1753, KL: 10.0686) Grad: 49.8823\n",
      "Step 3,500 (N samples: 448,000), Loss: 124.3095 (Recon: 114.2417, KL: 10.0678) Grad: 47.3497\n",
      "Step 3,600 (N samples: 460,800), Loss: 125.8395 (Recon: 115.6041, KL: 10.2354) Grad: 52.5773\n",
      "Step 3,700 (N samples: 473,600), Loss: 118.6670 (Recon: 108.6544, KL: 10.0126) Grad: 49.6700\n",
      "====> Test set loss: 123.4058 (BCE: 113.3891, KLD: 10.0167)\n",
      "Epoch 9/15\n",
      "Step 3,800 (N samples: 486,400), Loss: 119.0559 (Recon: 108.5789, KL: 10.4770) Grad: 45.7725\n",
      "Step 3,900 (N samples: 499,200), Loss: 119.9012 (Recon: 109.6797, KL: 10.2216) Grad: 51.4269\n",
      "Step 4,000 (N samples: 512,000), Loss: 118.9965 (Recon: 108.7348, KL: 10.2617) Grad: 51.2951\n",
      "Step 4,100 (N samples: 524,800), Loss: 125.1571 (Recon: 114.8358, KL: 10.3213) Grad: 48.4295\n",
      "Step 4,200 (N samples: 537,600), Loss: 128.0199 (Recon: 117.8873, KL: 10.1326) Grad: 44.2591\n",
      "====> Test set loss: 122.2720 (BCE: 111.9558, KLD: 10.3162)\n",
      "Epoch 10/15\n",
      "Step 4,300 (N samples: 550,400), Loss: 122.9932 (Recon: 112.7906, KL: 10.2026) Grad: 53.3283\n",
      "Step 4,400 (N samples: 563,200), Loss: 118.5021 (Recon: 108.1342, KL: 10.3679) Grad: 43.6832\n",
      "Step 4,500 (N samples: 576,000), Loss: 123.2465 (Recon: 112.7181, KL: 10.5284) Grad: 76.1373\n",
      "Step 4,600 (N samples: 588,800), Loss: 118.4538 (Recon: 108.0000, KL: 10.4537) Grad: 52.9025\n",
      "====> Test set loss: 121.9964 (BCE: 111.6651, KLD: 10.3312)\n",
      "Epoch 11/15\n",
      "Step 4,700 (N samples: 601,600), Loss: 119.8646 (Recon: 109.5242, KL: 10.3404) Grad: 42.6900\n",
      "Step 4,800 (N samples: 614,400), Loss: 120.0707 (Recon: 109.5570, KL: 10.5137) Grad: 47.0588\n",
      "Step 4,900 (N samples: 627,200), Loss: 120.1082 (Recon: 109.5710, KL: 10.5372) Grad: 42.1115\n",
      "Step 5,000 (N samples: 640,000), Loss: 125.0554 (Recon: 114.6738, KL: 10.3816) Grad: 44.1981\n",
      "Step 5,100 (N samples: 652,800), Loss: 121.8911 (Recon: 111.4524, KL: 10.4387) Grad: 57.4546\n",
      "====> Test set loss: 121.3149 (BCE: 110.7366, KLD: 10.5783)\n",
      "Epoch 12/15\n",
      "Step 5,200 (N samples: 665,600), Loss: 117.1315 (Recon: 106.6585, KL: 10.4730) Grad: 45.4016\n",
      "Step 5,300 (N samples: 678,400), Loss: 120.9209 (Recon: 110.2296, KL: 10.6913) Grad: 47.7681\n",
      "Step 5,400 (N samples: 691,200), Loss: 121.5999 (Recon: 111.1469, KL: 10.4530) Grad: 54.4839\n",
      "Step 5,500 (N samples: 704,000), Loss: 124.1228 (Recon: 113.5067, KL: 10.6160) Grad: 56.9570\n",
      "Step 5,600 (N samples: 716,800), Loss: 114.0981 (Recon: 103.6478, KL: 10.4502) Grad: 40.2664\n",
      "====> Test set loss: 121.1118 (BCE: 110.6276, KLD: 10.4842)\n",
      "Epoch 13/15\n",
      "Step 5,700 (N samples: 729,600), Loss: 124.0505 (Recon: 113.5648, KL: 10.4857) Grad: 55.9725\n",
      "Step 5,800 (N samples: 742,400), Loss: 127.1677 (Recon: 116.5218, KL: 10.6459) Grad: 47.2172\n",
      "Step 5,900 (N samples: 755,200), Loss: 122.4596 (Recon: 111.7776, KL: 10.6820) Grad: 53.8463\n",
      "Step 6,000 (N samples: 768,000), Loss: 118.2809 (Recon: 108.0539, KL: 10.2271) Grad: 47.2174\n",
      "====> Test set loss: 121.1460 (BCE: 110.7223, KLD: 10.4237)\n",
      "Epoch 14/15\n",
      "Step 6,100 (N samples: 780,800), Loss: 121.5361 (Recon: 111.1339, KL: 10.4022) Grad: 48.2883\n",
      "Step 6,200 (N samples: 793,600), Loss: 124.2524 (Recon: 113.6290, KL: 10.6234) Grad: 52.1750\n",
      "Step 6,300 (N samples: 806,400), Loss: 122.1390 (Recon: 111.4978, KL: 10.6412) Grad: 49.3353\n",
      "Step 6,400 (N samples: 819,200), Loss: 114.5237 (Recon: 104.0293, KL: 10.4945) Grad: 47.4036\n",
      "Step 6,500 (N samples: 832,000), Loss: 121.9311 (Recon: 111.4153, KL: 10.5159) Grad: 38.3600\n",
      "====> Test set loss: 120.1369 (BCE: 109.6072, KLD: 10.5296)\n",
      "Epoch 15/15\n",
      "Step 6,600 (N samples: 844,800), Loss: 121.6791 (Recon: 111.0594, KL: 10.6197) Grad: 42.9508\n",
      "Step 6,700 (N samples: 857,600), Loss: 122.3108 (Recon: 111.5669, KL: 10.7438) Grad: 60.0994\n",
      "Step 6,800 (N samples: 870,400), Loss: 116.3946 (Recon: 105.9077, KL: 10.4869) Grad: 55.3881\n",
      "Step 6,900 (N samples: 883,200), Loss: 121.5594 (Recon: 110.8804, KL: 10.6790) Grad: 56.1138\n",
      "Step 7,000 (N samples: 896,000), Loss: 118.1217 (Recon: 107.3826, KL: 10.7391) Grad: 45.8223\n",
      "====> Test set loss: 120.0317 (BCE: 109.4473, KLD: 10.5845)\n"
     ]
    }
   ],
   "source": [
    "prev_updates = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    prev_updates = train(model, train_loader, optimizer, prev_updates, writer=writer)\n",
    "    test(model, test_loader, prev_updates, writer=writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "In this notebook the whole preprocessing, training, and evaluation will take place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries\n",
    "\n",
    "Library | Version | Channel\n",
    "--- | --- | ---\n",
    "NumPy | 1.26.4 | default\n",
    "PyTorch | 2.2.2 | pytorch\n",
    "Torchvision | 0.17.2 | pytorch\n",
    "Tensorboard | / | conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The [MNIST](http://yann.lecun.com/exdb/mnist/) dataset is a widely-used benchmark in machine learning, consisting of 70,000 images of handwritten digits from 0 to 9. Each image is a 28x28 grayscale pixel grid. Due to its simplicity and well-structured format, MNIST serves as an excellent starting point for developing and testing machine learning models, particularly in the field of image recognition and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"../data/adata_normalized_sample.h5ad\"\n",
    "file_path = \"../data/adata_30kx10k_normalized_sample.h5ad\"\n",
    "\n",
    "adata = ad.read_h5ad(filename=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 30000 × 10000\n",
       "    obs: 'NRP', 'age_group', 'cell_source', 'cell_type', 'donor', 'gender', 'n_counts', 'n_genes', 'percent_mito', 'percent_ribo', 'region', 'sample', 'scrublet_score', 'source', 'type', 'version', 'cell_states', 'Used'\n",
       "    var: 'gene_ids-Harvard-Nuclei', 'feature_types-Harvard-Nuclei', 'gene_ids-Sanger-Nuclei', 'feature_types-Sanger-Nuclei', 'gene_ids-Sanger-Cells', 'feature_types-Sanger-Cells', 'gene_ids-Sanger-CD45', 'feature_types-Sanger-CD45'\n",
       "    uns: 'cell_type_colors'\n",
       "    obsm: 'X_pca', 'X_umap'\n",
       "    layers: 'cpm_normalized', 'min_max_normalized'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work yet --> Data type error\n",
    "full_data = adata.layers[\"min_max_normalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12490183 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split\n",
    "\n",
    "Split data in training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SparseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for sparse data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sparse_data):\n",
    "        self.sparse_data = sparse_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sparse_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if index >= len(self):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        \n",
    "        # Extract the row as a dense numpy array\n",
    "        row = self.sparse_data.getrow(index).toarray().squeeze()\n",
    "        \n",
    "        # Convert the row to a PyTorch tensor\n",
    "        return torch.tensor(row, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * full_data.shape[0])\n",
    "test_size = full_data.shape[0] - train_size\n",
    "\n",
    "torch.manual_seed(2406)\n",
    "perm = torch.randperm(full_data.shape[0])\n",
    "train_split, test_split = perm[:train_size], perm[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SparseDataset(full_data[train_split, :])\n",
    "test_data = SparseDataset(full_data[test_split, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "batch_size = 128\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure\n",
    "\n",
    "The **autoencoder** is comprised of two primary components: the **encoder** and the **decoder**. The encoder is responsible for reducing the dimensionality of the input tensor. The decoder, in turn, attempts to reconstruct the original input data from the reduced representation generated by the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cuda = True if device == \"cuda\" else False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Hyperparameters used for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layers = [\n",
    "    (10000, nn.ReLU()), \n",
    "    (6000, nn.ReLU()), \n",
    "    (3000, nn.ReLU()), \n",
    "    (1000, nn.ReLU()), \n",
    "    (200, nn.ReLU())\n",
    "]\n",
    "# Optimizer\n",
    "learning_rate = 1e-1\n",
    "weight_decay = 1e-8\n",
    "# Training\n",
    "folds = 5\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is divided into two sections: one for training and the other for validation following training. This division is referred to as a **\"fold\"**. The fold is created by extracting all cells from a single donor to ensure that the results are not influenced by any batch effects specific to that donor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AEOutput:\n",
    "    \"\"\"\n",
    "    Dataclass for AE output.\n",
    "\n",
    "    Attributes:\n",
    "        z_sample (torch.Tensor): The sampled value of the latent variable z.\n",
    "        x_recon (torch.Tensor): The reconstructed output from the VAE.\n",
    "        loss (torch.Tensor): The overall loss of the VAE.\n",
    "    \"\"\"\n",
    "\n",
    "    z_sample: torch.Tensor\n",
    "    x_recon: torch.Tensor\n",
    "    loss: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder (AE) class.\n",
    "\n",
    "    Args:\n",
    "        size_layers (list): Dimensionality of the layers.\n",
    "        loss_function (nn.Module): Loss function used for evaluation.\n",
    "        optimizer (nn.Module): Optimizer used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size_layers: list[tuple[int, nn.Module]],\n",
    "        criterion: nn.modules.loss._Loss,\n",
    "        learning_rate: float = 1e-1,\n",
    "        weight_decay: float = 1e-8,\n",
    "        optimizer: torch.optim.Optimizer = torch.optim.Adam,\n",
    "    ):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        ## Encoder architecture\n",
    "        self.encoder_layers = []\n",
    "        # Only iterate until second to last element\n",
    "        # --> Idx of last element called\n",
    "        for idx, (size, activation) in enumerate(size_layers[:-1]):\n",
    "            # While second to last element no reached\n",
    "            # --> Activation function in decoder\n",
    "            if idx < len(size_layers[:-1]) - 1:\n",
    "                self.encoder_layers.append(nn.Linear(size, size_layers[idx + 1][0]))\n",
    "\n",
    "                # Checks if activation is viable\n",
    "                if activation is not None:\n",
    "                    assert isinstance(\n",
    "                        activation, nn.Module\n",
    "                    ), f\"Activation should be of type {nn.Module}\"\n",
    "                    self.encoder_layers.append(activation)\n",
    "            else:\n",
    "                self.encoder_layers.append(nn.Linear(size, size_layers[idx + 1][0]))\n",
    "\n",
    "        self.encoder = nn.Sequential(*self.encoder_layers)\n",
    "\n",
    "        print(\"Constructed encoder...\")\n",
    "\n",
    "        ## Decoder archtitecture\n",
    "        # Reverse to build decoder (hourglass)\n",
    "        reversed_layers = list(reversed(size_layers))\n",
    "        self.decoder_layers = []\n",
    "        for idx, (size, activation) in enumerate(reversed_layers[:-1]):\n",
    "            # While second to last element no reached\n",
    "            # --> Activation function in encoder\n",
    "            if idx < len(reversed_layers[:-1]) - 1:\n",
    "                self.decoder_layers.append(nn.Linear(size, reversed_layers[idx + 1][0]))\n",
    "\n",
    "                # Checks if activation is viable\n",
    "                if activation is not None:\n",
    "                    assert isinstance(\n",
    "                        activation, nn.Module\n",
    "                    ), f\"Activation should be of type {nn.Module}\"\n",
    "                    self.decoder_layers.append(activation)\n",
    "            else:\n",
    "                self.decoder_layers.append(nn.Linear(size, reversed_layers[idx + 1][0]))\n",
    "\n",
    "        self.decoder = nn.Sequential(*self.decoder_layers)\n",
    "\n",
    "        print(\"Constructed decoder...\")\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer(\n",
    "            params=self.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encodes the input data into the latent space.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input data.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Input data compressed to latent space.\n",
    "        \"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decodes the data from the latent space to the original input space.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): Data in the latent space.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed data in the original input space.\n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, compute_loss: bool = True):\n",
    "        \"\"\"\n",
    "        Performs a forward pass of the AE.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input data.\n",
    "            compute_loss (bool): Whether to compute the loss or not.\n",
    "\n",
    "        Returns:\n",
    "            VAEOutput: VAE output dataclass.\n",
    "        \"\"\"\n",
    "        z = self.encode(x)\n",
    "        recon_x = self.decode(z)\n",
    "\n",
    "        if not compute_loss:\n",
    "            return AEOutput(z_sample=z, x_recon=recon_x, loss=None)\n",
    "\n",
    "        # compute loss terms\n",
    "        loss_recon = self.criterion(recon_x, x)\n",
    "\n",
    "        return AEOutput(z_sample=z, x_recon=recon_x, loss=loss_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed encoder...\n",
      "Constructed decoder...\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder(size_layers, criterion=nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=10000, out_features=6000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=6000, out_features=3000, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=3000, out_features=1000, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=1000, out_features=200, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=1000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1000, out_features=3000, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=3000, out_features=6000, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=6000, out_features=10000, bias=True)\n",
       "  )\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer, prev_updates, writer=None, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Trains the model on the given data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        loss_fn: The loss function.\n",
    "        optimizer: The optimizer.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_idx, data in enumerate(tqdm(dataloader)):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        output = model(data)  # Forward pass\n",
    "        loss = output.loss\n",
    "\n",
    "        # loss.backward()\n",
    "\n",
    "        if n_upd % 100 == 0:\n",
    "            # Calculate and log gradient norms\n",
    "            total_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1.0 / 2)\n",
    "\n",
    "            print(\n",
    "                f\"Step {n_upd:,} (N samples: {n_upd * dataloader.batch_size:,}), Loss: {loss.item():.4f} Grad: {total_norm:.4f}\"\n",
    "            )\n",
    "\n",
    "            if writer is not None:\n",
    "                global_step = n_upd\n",
    "                writer.add_scalar(\"Loss/Train\", loss.item(), global_step)\n",
    "                writer.add_scalar(\"GradNorm/Train\", total_norm, global_step)\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()  # Update the model parameters\n",
    "\n",
    "    return prev_updates + len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, cur_step, writer=None, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Tests the model on the given data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to test.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        cur_step (int): The current step.\n",
    "        writer: The TensorBoard writer.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader, desc=\"Testing\"):\n",
    "            data = data.to(device)\n",
    "\n",
    "            output = model(data, compute_loss=True)  # Forward pass\n",
    "\n",
    "            test_loss += output.loss.item()\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    print(f\"====> Test set loss: {test_loss:.4f}\")\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_scalar(\"Loss/Test\", test_loss, global_step=cur_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "writer = SummaryWriter(f'runs/mnist/vae_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "\n",
    "prev_updates = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    prev_updates = train(model, train_loader, model.optimizer, prev_updates, writer=writer)\n",
    "    test(model, test_loader, prev_updates, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(train_data):\n",
    "    donors = adata.obs[\"donor\"].unique()\n",
    "\n",
    "    for donor in donors:\n",
    "        # Create training data\n",
    "        # Remove cells from chosen donor\n",
    "        train_data = adata[adata.obs.donor != donor]\n",
    "        # Create validation data\n",
    "        val_data = adata[adata.obs.donor == donor]\n",
    "\n",
    "    # Load data\n",
    "    train_loader = AnnLoader(train_data, batch_size=batch_size, shuffle=True, use_cuda=cuda)\n",
    "    val_loader = AnnLoader(val_data, batch_size=batch_size, shuffle=True, use_cuda=cuda)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Autoencoders are powerful neural network architectures used for unsupervised learning, enabling the extraction of meaningful features from high-dimensional datasets such as single-cell RNA sequencing (scRNA-seq) data. When applied to scRNA-seq data from the Heart Cell Atlas, autoencoders can efficiently compress and reconstruct gene expression profiles, aiding in the identification of cellular heterogeneity and underlying biological patterns in cardiac tissues.\n",
    "> Generated by ChatGPT\n",
    "\n",
    "In this notebook the whole preprocessing, training, and evaluation will take place.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries\n",
    "\n",
    "Library | Version | Channel\n",
    "--- | --- | ---\n",
    "anndata | 0.7.0 | bioconda?\n",
    "PyTorch | 2.2.2 | pytorch\n",
    "Torchvision | 0.17.2 | pytorch\n",
    "Tensorboard | / | conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import anndata as ad\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Get the absolute path of the 'notebooks' directory\n",
    "notebooks_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Construct the path to the 'src' directory\n",
    "src_path = os.path.abspath(os.path.join(notebooks_dir, \"..\", \"src\"))\n",
    "\n",
    "# Add the 'src' directory to the Python path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Self-build modules\n",
    "import autoencoder.ae_model as ae\n",
    "import autoencoder.ae_training as T\n",
    "import utils.data_utils as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model structure and hyperparameters from JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../config/autoencoder_test.json\"\n",
    "model_params = utils.import_model_params(file_path)\n",
    "\n",
    "model_architecture = model_params[\"model\"]\n",
    "model_training = model_params[\"training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = model_training[\"batch_size\"]  # Power of 2 is optimized in many libraries\n",
    "\n",
    "# Training\n",
    "num_epochs = model_training[\"training_epochs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Specification\n",
    "\n",
    "The CUDA architecture from NVIDIA enables high-performance parallel computing on GPUs, optimizing tasks through concurrent execution and accelerating applications like deep learning and scientific simulations.\n",
    "> Generated by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Established the type of device used for model processing\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cuda = True if device == \"cuda\" else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Annotated data in the h5ad format is widely used for efficiently storing and accessing large-scale single-cell RNA sequencing (scRNA-seq) datasets. Leveraging the anndata library, researchers can seamlessly manipulate and analyze these datasets, facilitating tasks such as data integration, preprocessing, and visualization, thereby enhancing insights into complex biological systems.\n",
    "> Generated by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/adata_normalized_sample.h5ad\"\n",
    "# file_path = \"../data/adata_30kx10k_normalized_sample.h5ad\"\n",
    "\n",
    "adata = ad.read_h5ad(filename=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1000 × 1000\n",
       "    obs: 'NRP', 'age_group', 'cell_source', 'cell_type', 'donor', 'gender', 'n_counts', 'n_genes', 'percent_mito', 'percent_ribo', 'region', 'sample', 'scrublet_score', 'source', 'type', 'version', 'cell_states', 'Used'\n",
       "    var: 'gene_ids-Harvard-Nuclei', 'feature_types-Harvard-Nuclei', 'gene_ids-Sanger-Nuclei', 'feature_types-Sanger-Nuclei', 'gene_ids-Sanger-Cells', 'feature_types-Sanger-Cells', 'gene_ids-Sanger-CD45', 'feature_types-Sanger-CD45'\n",
       "    uns: 'cell_type_colors'\n",
       "    obsm: 'X_pca', 'X_umap'\n",
       "    layers: 'cpm_normalized', 'min_max_normalized'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_data = adata.layers[\"min_max_normalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 39019 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split\n",
    "\n",
    "Split data in training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * count_data.shape[0])\n",
    "test_size = count_data.shape[0] - train_size\n",
    "\n",
    "torch.manual_seed(2406)\n",
    "perm = torch.randperm(count_data.shape[0])\n",
    "train_split, test_split = perm[:train_size], perm[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SparseDataset(count_data[train_split, :])\n",
    "test_data = SparseDataset(count_data[test_split, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<modules.sparse_dataset.SparseDataset at 0x7f8926da9c30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<modules.sparse_dataset.SparseDataset at 0x7f8926dab2e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure\n",
    "\n",
    "The **autoencoder** is comprised of two primary components: the **encoder** and the **decoder**. The encoder is responsible for reducing the dimensionality of the input tensor. The decoder, in turn, attempts to reconstruct the original input data from the reduced representation generated by the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layers, decoder_layers = utils.import_model_architecture(\n",
    "    forward=model_architecture[\"layers\"][\"encoder\"],\n",
    "    backward=model_architecture[\"layers\"][\"decoder\"],\n",
    ")\n",
    "\n",
    "loss_function = utils.import_loss_function(model_architecture[\"loss_function\"])\n",
    "\n",
    "model = ae.Autoencoder(encoder_layers, decoder_layers, loss_function=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=600, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "    (3): SiLU()\n",
       "    (4): Linear(in_features=300, out_features=50, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=300, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=300, out_features=600, bias=True)\n",
       "    (3): SiLU()\n",
       "    (4): Linear(in_features=600, out_features=1000, bias=True)\n",
       "  )\n",
       "  (loss_function): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:00<00:00,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (N samples: 0), Loss: 0.0008 Grad: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 19.82it/s]\n",
      "Testing: 100%|██████████| 2/2 [00:00<00:00, 58.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0002\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 25.60it/s]\n",
      "Testing: 100%|██████████| 2/2 [00:00<00:00, 48.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0001\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 17.73it/s]\n",
      "Testing: 100%|██████████| 2/2 [00:00<00:00, 53.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0001\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 20.23it/s]\n",
      "Testing: 100%|██████████| 2/2 [00:00<00:00, 40.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0001\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 24.72it/s]\n",
      "Testing: 100%|██████████| 2/2 [00:00<00:00, 55.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = utils.import_optimizer(\n",
    "    model.parameters(),\n",
    "    model_architecture[\"optimization\"][\"optimizer\"],\n",
    "    learning_rate=model_architecture[\"optimization\"][\"learning_rate\"],\n",
    "    weight_decay=model_architecture[\"optimization\"][\"weight_decay\"],\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(\n",
    "    f'../runs/hca/ae_{num_epochs}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    ")\n",
    "\n",
    "prev_updates = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    prev_updates = T.train(\n",
    "        model, train_loader, optimizer, prev_updates, device, writer=writer\n",
    "    )\n",
    "    T.test(model, test_loader, prev_updates, device=device, writer=writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

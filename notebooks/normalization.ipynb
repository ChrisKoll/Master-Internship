{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization\n",
    "\n",
    "This notebook outlines a range of normalization methodologies tailored specifically for enhancing the computational processing of single-cell RNA sequencing (scRNA-seq) data. In the realm of scRNA-seq, molecular profiling at the level of individual cells facilitates profound insights into gene expression patterns.\n",
    "\n",
    "*Employed methods:*\n",
    "\n",
    "- CPM normalization\n",
    "- Log normalization\n",
    "- Min-Max normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "Library | Version | Channel\n",
    "--- | --- | ---\n",
    "NumPy | 1.26.4 | Default\n",
    "RNAnorm | 2.1.0 | Bioconda\n",
    "SciPy | 1.12.0 | Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnanorm import CPM\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import modules.normalization as norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Matix\n",
    "\n",
    "All example calculations are performed with this matrix.  \n",
    "The efficacy of normalization methods is firstly established through testing on a dense matrix representation. Furthermore, in order to ensure the robustness of these methods across various data formats, they are also evaluated using sparse matrix representations, given the sparse format of the **h5ad** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non zero row indices\n",
    "row = np.array([5, 8, 2, 3, 1, 7, 0, 4, 6, 9, 2, 3, 8, 1, 0])\n",
    "# Non zero col indicies\n",
    "col = np.array([4, 3, 2, 0, 1, 2, 0, 3, 1, 4, 3, 2, 4, 3, 1])\n",
    "# Non zero data\n",
    "data = np.array([56, 183, 109, 24, 71, 145, 92, 12, 176, 31, 198, 64, 37, 115, 82])\n",
    "\n",
    "# Creates sparse matrix with 10 rows (cells) and 5 cols (genes)\n",
    "test_sparse = sp.csr_matrix((data, (row, col)), shape=(10, 5))\n",
    "# Dense matrix\n",
    "test_dense = test_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts per Million (CPM)\n",
    "\n",
    "To compute the Counts Per Million (CPM) for a gene in a sample, the counts are scaled by a factor reflecting a million mapped reads to ensure comparability. Then, they're normalized by dividing through the total mapped reads in the sample to facilitate meaningful expression level comparisons [[1]](https://www.reneshbedre.com/blog/expression_units.html).\n",
    "In mathematical terms, the formula looks like this:  \n",
    "\n",
    "$$CPM_{ij} = \\frac{count\\ gene\\ j * 10^6}{\\sum{counts\\ sample\\ i}}$$  \n",
    "\n",
    "**Note:** Gene length is not considered during normalization. For this analysis gene length seems not important.\n",
    "\n",
    "When analyzing scRNA-seq data, the focus is not on comparing the expression levels of different genes against each other, but rather on comparing the expression patterns across different cells. Consequently, the calculation formula is adjusted to suit this objective:  \n",
    "\n",
    "$$CPM_{ij} = \\frac{count\\ gene\\ j * 10^6}{\\sum{counts\\ cell\\ i}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example - Sanity\n",
    "\n",
    "Using the CPM function from the RNAnorm library [[2]](https://github.com/genialis/RNAnorm?tab=readme-ov-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 528735.63218391,  471264.36781609,       0.        ,\n",
       "              0.        ,       0.        ],\n",
       "       [      0.        ,  381720.43010753,       0.        ,\n",
       "         618279.56989247,       0.        ],\n",
       "       [      0.        ,       0.        ,  355048.85993485,\n",
       "         644951.14006515,       0.        ],\n",
       "       [ 272727.27272727,       0.        ,  727272.72727273,\n",
       "              0.        ,       0.        ],\n",
       "       [      0.        ,       0.        ,       0.        ,\n",
       "        1000000.        ,       0.        ],\n",
       "       [      0.        ,       0.        ,       0.        ,\n",
       "              0.        , 1000000.        ],\n",
       "       [      0.        , 1000000.        ,       0.        ,\n",
       "              0.        ,       0.        ],\n",
       "       [      0.        ,       0.        , 1000000.        ,\n",
       "              0.        ,       0.        ],\n",
       "       [      0.        ,       0.        ,       0.        ,\n",
       "         831818.18181818,  168181.81818182],\n",
       "       [      0.        ,       0.        ,       0.        ,\n",
       "              0.        , 1000000.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_result = CPM().fit_transform(test_dense)\n",
    "sanity_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example - Dense\n",
    "\n",
    "CPM normalization using the dense matrix as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 528735.63218391,  471264.36781609,       0.        ,\n",
       "              0.        ,       0.        ],\n",
       "       [      0.        ,  381720.43010753,       0.        ,\n",
       "         618279.56989247,       0.        ],\n",
       "       [      0.        ,       0.        ,  355048.85993485,\n",
       "         644951.14006515,       0.        ],\n",
       "       [ 272727.27272727,       0.        ,  727272.72727273,\n",
       "              0.        ,       0.        ],\n",
       "       [      0.        ,       0.        ,       0.        ,\n",
       "        1000000.        ,       0.        ],\n",
       "       [      0.        ,       0.        ,       0.        ,\n",
       "              0.        , 1000000.        ],\n",
       "       [      0.        , 1000000.        ,       0.        ,\n",
       "              0.        ,       0.        ],\n",
       "       [      0.        ,       0.        , 1000000.        ,\n",
       "              0.        ,       0.        ],\n",
       "       [      0.        ,       0.        ,       0.        ,\n",
       "         831818.18181818,  168181.81818182],\n",
       "       [      0.        ,       0.        ,       0.        ,\n",
       "              0.        , 1000000.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_result = norm.dense_cpm(test_dense)\n",
    "dense_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example - Sparse\n",
    "\n",
    "In order to implement the divisions of values for the sparse matrix, a diagonal matrix of the reciprocals of the row sums was created. By multiplying this diagonal matrix, the division of the values could be emulated [[3]](https://stackoverflow.com/questions/42225269/scipy-sparse-matrix-division)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 528735.63218391,  471264.36781609,       0.        ,\n",
       "              0.        ,       0.        ],\n",
       "       [      0.        ,  381720.43010753,       0.        ,\n",
       "         618279.56989247,       0.        ],\n",
       "       [      0.        ,       0.        ,  355048.85993485,\n",
       "         644951.14006515,       0.        ],\n",
       "       [ 272727.27272727,       0.        ,  727272.72727273,\n",
       "              0.        ,       0.        ],\n",
       "       [      0.        ,       0.        ,       0.        ,\n",
       "        1000000.        ,       0.        ],\n",
       "       [      0.        ,       0.        ,       0.        ,\n",
       "              0.        , 1000000.        ],\n",
       "       [      0.        , 1000000.        ,       0.        ,\n",
       "              0.        ,       0.        ],\n",
       "       [      0.        ,       0.        , 1000000.        ,\n",
       "              0.        ,       0.        ],\n",
       "       [      0.        ,       0.        ,       0.        ,\n",
       "         831818.18181818,  168181.81818182],\n",
       "       [      0.        ,       0.        ,       0.        ,\n",
       "              0.        , 1000000.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_result = norm.sparse_cpm(test_sparse)\n",
    "sparse_result.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "&rarr; **All 3 functions appear to compute the same results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-0-Max-1 Normalization\n",
    "\n",
    "Min-Max normalization is a normalization typically used in machine learning.  \n",
    "With this normalization method the values are normalized so that the lowest value in the dataset is the **min** value and the highest value is **max** [[3]](https://www.datacamp.com/tutorial/normalization-in-machine-learning).  \n",
    "The formula for this methods looks like this:  \n",
    "$$X_{normalized} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$  \n",
    "\n",
    "**Note:** This functionality is also implemented in the Python library *sklearn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_max(data, min_val=0, max_val=1):\n",
    "    min_data = np.min(data)\n",
    "    max_data = np.max(data)\n",
    "\n",
    "    # If min and max are not set to 0 and 1 this is the full formula\n",
    "    min_max_normalized = (data - min_data) / (max_data - min_data) * (max_val - min_val) + min_val\n",
    "\n",
    "    return min_max_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_matrix = example_result\n",
    "print(example_matrix)\n",
    "\n",
    "example_result_2 = calculate_min_max(example_matrix)\n",
    "example_result_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Min-Max method has extra calculations. These are added in this function to provide full functionality if the min and max values are **not 0 and 1**. If this is not the case, the steps are not needed. If the boundaries for the data **are 0 and 1**:  \n",
    "$$bound_{max} - bound_{min}$$ \n",
    "- Turns out to 1 - 0 = 1 (result multiplied with 1)\n",
    "$$bound_{min}$$\n",
    "- Turns out to 0 (adds 0 to result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caclulate_sparse_min_max(data, min_val=0, max_val=1):\n",
    "    # Get min value from sparse matrix\n",
    "    min_data = data.min()\n",
    "    # Get max value from sparse matrix\n",
    "    max_data = data.max()\n",
    "\n",
    "    # Calculate Min-Max as described above\n",
    "    min_max_matrix = (cpm_normalized - min_data) / (max_data - min_data) * (max_val - min_val) + min_val\n",
    "\n",
    "    return min_max_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_normalized = caclulate_sparse_min_max(cpm_normalized)\n",
    "min_max_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Save the results as additional layers in the anndata object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers[\"cpm_normalized\"] = cpm_normalized\n",
    "adata.layers[\"min_max_normalized\"] = min_max_normalized\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_h5ad(filename=\"../data/adata_30kx10k_normalized_sample.h5ad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

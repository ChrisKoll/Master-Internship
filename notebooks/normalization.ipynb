{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization\n",
    "\n",
    "This notebook outlines a range of normalization methodologies tailored specifically for enhancing the computational processing of single-cell RNA sequencing (scRNA-seq) data. In the realm of scRNA-seq, molecular profiling at the level of individual cells facilitates profound insights into gene expression patterns.\n",
    "\n",
    "*Employed methods:*\n",
    "\n",
    "- CPM normalization\n",
    "- Log normalization\n",
    "- Min-Max normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "Library | Version | Channel\n",
    "--- | --- | ---\n",
    "NumPy | 1.26.4 | Default\n",
    "RNAnorm | 2.1.0 | Bioconda\n",
    "Scikit-Learn | 1.4.2 | Default\n",
    "SciPy | 1.12.0 | Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnanorm import CPM\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler\n",
    "\n",
    "import modules.normalization as norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Matix\n",
    "\n",
    "All example calculations are performed with this matrix.  \n",
    "The efficacy of normalization methods is firstly established through testing on a dense matrix representation. Furthermore, in order to ensure the robustness of these methods across various data formats, they are also evaluated using sparse matrix representations, given the sparse format of the **h5ad** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non zero row indices\n",
    "row = np.array([5, 8, 2, 3, 1, 7, 0, 4, 6, 9, 2, 3, 8, 1, 0])\n",
    "# Non zero col indicies\n",
    "col = np.array([4, 3, 2, 0, 1, 2, 0, 3, 1, 4, 3, 2, 4, 3, 1])\n",
    "# Non zero data\n",
    "data = np.array([56, 183, 109, 24, 71, 145, 92, 12, 176, 31, 198, 64, 37, 115, 82])\n",
    "\n",
    "# Creates sparse matrix with 10 rows (cells) and 5 cols (genes)\n",
    "test_sparse = sp.csr_matrix((data, (row, col)), shape=(10, 5))\n",
    "# Dense matrix\n",
    "test_dense = test_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts per Million (CPM)\n",
    "\n",
    "To compute the Counts Per Million (CPM) for a gene in a sample, the counts are scaled by a factor reflecting a million mapped reads to ensure comparability. Then, they're normalized by dividing through the total mapped reads in the sample to facilitate meaningful expression level comparisons [[1]](https://www.reneshbedre.com/blog/expression_units.html).\n",
    "In mathematical terms, the formula looks like this:  \n",
    "\n",
    "$$CPM_{ij} = \\frac{count\\ gene\\ j * 10^6}{\\sum{counts\\ sample\\ i}}$$  \n",
    "\n",
    "**Note:** Gene length is not considered during normalization. For this analysis gene length seems not important.\n",
    "\n",
    "When analyzing scRNA-seq data, the focus is not on comparing the expression levels of different genes against each other, but rather on comparing the expression patterns across different cells. Consequently, the calculation formula is adjusted to suit this objective:  \n",
    "\n",
    "$$CPM_{ij} = \\frac{count\\ gene\\ j * 10^6}{\\sum{counts\\ cell\\ i}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example - Sanity\n",
    "\n",
    "Sanity check for the correctness of the self-implemented CPM function.  \n",
    "Using the CPM function from the RNAnorm library [[2]](https://github.com/genialis/RNAnorm?tab=readme-ov-file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm_sanity = CPM().fit_transform(test_dense)\n",
    "cpm_sanity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example - Dense\n",
    "\n",
    "CPM normalization using the dense matrix as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm_dense = norm.dense_cpm(test_dense)\n",
    "cpm_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example - Sparse\n",
    "\n",
    "In order to implement the divisions of values for the sparse matrix, a diagonal matrix of the reciprocals of the row sums was created. By multiplying this diagonal matrix, the division of the values could be emulated [[3]](https://stackoverflow.com/questions/42225269/scipy-sparse-matrix-division)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm_sparse = norm.sparse_cpm(test_sparse)\n",
    "\n",
    "# Print as dense matrix\n",
    "cpm_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "&rarr; **All 3 functions appear to compute the same results.**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transformation\n",
    "\n",
    "Log transformation is feature transformation technique. We apply the natural logarithm to each value of the matrix. This reduces the impact of outliers and enhance the fitting of the model [[4]](https://www.pythonprog.com/log-transformation-in-machine-learning/).  \n",
    "**1** is added to each value to achive a good transformation for all 0 values.\n",
    "\n",
    "$$\\log(0) = NaN$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example - Sanity\n",
    "\n",
    "Sanity check for the correctness of the self-implemented log function.  \n",
    "Used implemented functionality from the Scikit-Learn package [[5]](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log transformer\n",
    "log_transformer = FunctionTransformer(np.log)\n",
    "\n",
    "# Apply transformation\n",
    "log_sanity = log_transformer.transform(test_dense + 1)\n",
    "log_sanity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example - Dense\n",
    "\n",
    "Log transformation with a dense matrix as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dense = norm.dense_log(test_dense)\n",
    "log_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example - Sparse\n",
    "\n",
    "The CSR class from SciPy already includes a log transformation method that also adds 1 to each value in order to prevent NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sparse = test_sparse.log1p()\n",
    "log_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "&rarr; **All 3 functions appear to compute the same results.**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-0-Max-1 Normalization\n",
    "\n",
    "Min-Max normalization is a normalization typically used in machine learning.  \n",
    "With this normalization method the values are normalized so that the lowest value in the dataset is the **min** value and the highest value is **max** [[6]](https://www.datacamp.com/tutorial/normalization-in-machine-learning).  \n",
    "The formula for this methods looks like this:  \n",
    "$$X_{ij} = \\frac{X_{ij} - X_{min}}{X_{max} - X_{min}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example - Sanity\n",
    "\n",
    "Sanity check for testing the correctness of the self-implemented min-max function.  \n",
    "Used the implemented functionality of the Scikit-Learn package [[7]](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.46590909, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.40340909, 0.        , 0.58080808, 0.        ],\n",
       "       [0.        , 0.        , 0.75172414, 1.        , 0.        ],\n",
       "       [0.26086957, 0.        , 0.44137931, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.06060606, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.92424242, 0.66071429],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.55357143]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define min-max scaler\n",
    "min_max_transformer = MinMaxScaler().fit(test_dense)\n",
    "\n",
    "# Apply tranformation\n",
    "min_max_sanity = min_max_transformer.transform(test_dense)\n",
    "min_max_sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 92,  82,   0,   0,   0],\n",
       "       [  0,  71,   0, 115,   0],\n",
       "       [  0,   0, 109, 198,   0],\n",
       "       [ 24,   0,  64,   0,   0],\n",
       "       [  0,   0,   0,  12,   0],\n",
       "       [  0,   0,   0,   0,  56],\n",
       "       [  0, 176,   0,   0,   0],\n",
       "       [  0,   0, 145,   0,   0],\n",
       "       [  0,   0,   0, 183,  37],\n",
       "       [  0,   0,   0,   0,  31]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example - Dense\n",
    "\n",
    "Min-Max normalization with the dense matrix as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_dense = norm.dense_min_max(test_dense)\n",
    "min_max_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dense"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
